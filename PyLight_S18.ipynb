{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40891,"status":"ok","timestamp":1717808523095,"user":{"displayName":"Shiv Dutta","userId":"07673039161083621542"},"user_tz":-330},"id":"H3feyUlMOVkN","outputId":"ddc22f8c-fd76-48a2-96e1-319a29217eec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab_Notebooks/Session18\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Colab_Notebooks/Session18"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":66575,"status":"ok","timestamp":1717808589666,"user":{"displayName":"Shiv Dutta","userId":"07673039161083621542"},"user_tz":-330},"id":"lBunYecQsUO8","outputId":"674aa0ea-da2e-4449-af00-b9bdf23bfb56"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchmetrics\n","  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/868.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.8/868.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m573.4/868.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets\n","  Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n","Collecting pytorch-lightning\n","  Downloading pytorch_lightning-2.2.5-py3-none-any.whl (802 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.3/802.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting lion-pytorch\n","  Downloading lion_pytorch-0.1.4-py3-none-any.whl (4.3 kB)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.3.0+cu121)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n","  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Collecting requests>=2.32.1 (from datasets)\n","  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2024.6.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->torchmetrics)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->torchmetrics)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->torchmetrics)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->torchmetrics)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->torchmetrics)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->torchmetrics)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->torchmetrics)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n","Installing collected packages: xxhash, requests, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, torchmetrics, lion-pytorch, pytorch-lightning\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.31.0\n","    Uninstalling requests-2.31.0:\n","      Successfully uninstalled requests-2.31.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-2.19.2 dill-0.3.8 lightning-utilities-0.11.2 lion-pytorch-0.1.4 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pytorch-lightning-2.2.5 requests-2.32.3 torchmetrics-1.4.0.post0 xxhash-3.4.1\n"]}],"source":["!pip install torchmetrics datasets tokenizers pytorch-lightning lion-pytorch -U"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3925,"status":"ok","timestamp":1717808593587,"user":{"displayName":"Shiv Dutta","userId":"07673039161083621542"},"user_tz":-330},"id":"ijC3uu3nT5gV"},"outputs":[],"source":["import sys\n","import warnings\n","\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import ModelSummary, LearningRateMonitor, ModelCheckpoint, LearningRateFinder\n","\n","warnings.filterwarnings(\"ignore\")\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":980,"status":"ok","timestamp":1717808594562,"user":{"displayName":"Shiv Dutta","userId":"07673039161083621542"},"user_tz":-330},"id":"x0IUC5HeQJCc"},"outputs":[],"source":["from config import get_config\n","\n","cfg = get_config()\n","\n","cfg['batch_size'] = 6\n","cfg['preload'] = None\n","cfg['num_epochs'] = 18"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["d0c593a377984bc0ac68bb3d8fa0225b","a14dcc7202cf4a49a5950f65f2cacd8a","a29fb6ca694a4af3b0d451accb12cd9b","1c921d1bbe2a4ef1871f619751e67c29","ec26cc15d19d4380a66f1348c0b3f6ae","1db5a93b4a1a4408aa4b77f1958bfb23","8a604d4faa044327bf9a87d7a050623e","129b1c06fc1d4e3fbd07352ae558ea58","0d6c22dcc6114cc585579399f8adbcce","076fcb0dbe3c46c8a5e62969a07266c1","7cd4ba7696d7454cbaaa7672edc08faf","f7839011b87f47b8b1a969fc2c29c959","defe3e10fae3405c8432e4a75c8adf36","57410fc14d0d4e7eb8216b9a4853070c","d09e87d610b249ff98b23b2b8a6ca527","75ef97bef3b2439392a58c2613b5b3e7","9d0d9d55cb584d648dee1735970370f8","d6febfc9cfa747bbacfb7c46cb2148bf","23107d08b809488cb886fdfb746446b2","5446b01d32a24c96bb1ae3254442f645","f6fd9b85203a41178ee625f241893c84","af54adbdcfaa4aecaa918cc3675a4c7e","f007c7e054664ede8328f343346aa808","81281a22d5414dbcb1985acd5d934da5","838a1cbdd0db454eb991934039a70be9","bc0b8b0f34b64402bfe0afb5eb41c202","fbc847307db246beb77f71b1292e9010","e076052d62914452bae270677e31cd59","19a46868b2444eb4869b16cf346e4d3e","f78ecd1aa75144aa86e5330fb072dd46","6893b61500724731b8064b6f1441f651","fe411b0dfdf94fe7aba649c28148c928","684c5246fe6f4f8dac202c5b1514d5e6"]},"executionInfo":{"elapsed":13476,"status":"ok","timestamp":1717808608035,"user":{"displayName":"Shiv Dutta","userId":"07673039161083621542"},"user_tz":-330},"id":"xFWQDCMrWRwQ","outputId":"0adb2065-1fe2-439c-89c6-cb288e9ad489"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading readme:   0%|          | 0.00/28.1k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0c593a377984bc0ac68bb3d8fa0225b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/5.73M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7839011b87f47b8b1a969fc2c29c959"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/32332 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f007c7e054664ede8328f343346aa808"}},"metadata":{}}],"source":["from LITTransformer import LITTransformer\n","\n","model = LITTransformer(cfg)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5d3a2dad379c43239a9690e544781ea9","9e77971ee2e2438c9badb8da3e6bccea","94a42c5391884245b356645f4cc1bcce","7213a68c5d9448deb6f4f74e71a3d672","099e58da9061422b9e61cc8013800173","0437f15650b345058b57c23047c79f36","db41403862d241d5aff26b42d7a2ea6d","1f0ee045172f47bd8153f6634f611b78","2c49c953ada04b0b9dbf9d2e5cf42db5","5a7995d8bdd84781a553c31fade773f7","a7af00a888c34b95ad28fea34d3cf4d2","39c7514e76114f778d5d23c4112e4d61","c96443ab9a7245e5ac17c4ff203cb979","3cdb589de67b4c21a961a901cead2b35","ad0db756198e4dd1a37ca9326add3796","cfc7f9a927624aa7ab0a3c60b5bce3b1","c68799d28b5c41eca40f1123ce310b90","c2f7df85960649dc827a451658e17379","c8a171c747c94287a8278af90f416d2c","71fa4dbb11464937a2198862745b3e7b","e9997997b56341f882bc9c9d14cf776d","eb54799ccfc14e3da35dca0539de3eef"]},"id":"b6vE3PV0T7TG","outputId":"e5a04daf-01dc-417a-da79-8f8287c19a10"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n","INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n","INFO:pytorch_lightning.utilities.rank_zero:`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n","INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA L4') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"output_type":"stream","name":"stdout","text":["Max length of source sentence: 309\n","Max length of target sentence: 274\n"]},{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.callbacks.model_summary:\n","    | Name                                                  | Type                    | Params\n","----------------------------------------------------------------------------------------------------\n","0   | model                                                 | Transformer             | 75.1 M\n","1   | model.encoder                                         | Encoder                 | 18.9 M\n","2   | model.encoder.layers                                  | ModuleList              | 18.9 M\n","3   | model.encoder.layers.0                                | EncoderBlock            | 3.1 M \n","4   | model.encoder.layers.0.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n","5   | model.encoder.layers.0.self_attention_block.w_q       | Linear                  | 262 K \n","6   | model.encoder.layers.0.self_attention_block.w_k       | Linear                  | 262 K \n","7   | model.encoder.layers.0.self_attention_block.w_v       | Linear                  | 262 K \n","8   | model.encoder.layers.0.self_attention_block.w_o       | Linear                  | 262 K \n","9   | model.encoder.layers.0.self_attention_block.dropout   | Dropout                 | 0     \n","10  | model.encoder.layers.0.feed_forward_block             | FeedForwardBlock        | 2.1 M \n","11  | model.encoder.layers.0.feed_forward_block.linear_1    | Linear                  | 1.1 M \n","12  | model.encoder.layers.0.feed_forward_block.dropout     | Dropout                 | 0     \n","13  | model.encoder.layers.0.feed_forward_block.linear_2    | Linear                  | 1.0 M \n","14  | model.encoder.layers.0.residual_connections           | ModuleList              | 4     \n","15  | model.encoder.layers.0.residual_connections.0         | ResidualConnection      | 2     \n","16  | model.encoder.layers.0.residual_connections.0.dropout | Dropout                 | 0     \n","17  | model.encoder.layers.0.residual_connections.0.norm    | LayerNormalization      | 2     \n","18  | model.encoder.layers.0.residual_connections.1         | ResidualConnection      | 2     \n","19  | model.encoder.layers.0.residual_connections.1.dropout | Dropout                 | 0     \n","20  | model.encoder.layers.0.residual_connections.1.norm    | LayerNormalization      | 2     \n","21  | model.encoder.layers.1                                | EncoderBlock            | 3.1 M \n","22  | model.encoder.layers.1.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n","23  | model.encoder.layers.1.self_attention_block.w_q       | Linear                  | 262 K \n","24  | model.encoder.layers.1.self_attention_block.w_k       | Linear                  | 262 K \n","25  | model.encoder.layers.1.self_attention_block.w_v       | Linear                  | 262 K \n","26  | model.encoder.layers.1.self_attention_block.w_o       | Linear                  | 262 K \n","27  | model.encoder.layers.1.self_attention_block.dropout   | Dropout                 | 0     \n","28  | model.encoder.layers.1.feed_forward_block             | FeedForwardBlock        | 2.1 M \n","29  | model.encoder.layers.1.feed_forward_block.linear_1    | Linear                  | 1.1 M \n","30  | model.encoder.layers.1.feed_forward_block.dropout     | Dropout                 | 0     \n","31  | model.encoder.layers.1.feed_forward_block.linear_2    | Linear                  | 1.0 M \n","32  | model.encoder.layers.1.residual_connections           | ModuleList              | 4     \n","33  | model.encoder.layers.1.residual_connections.0         | ResidualConnection      | 2     \n","34  | model.encoder.layers.1.residual_connections.0.dropout | Dropout                 | 0     \n","35  | model.encoder.layers.1.residual_connections.0.norm    | LayerNormalization      | 2     \n","36  | model.encoder.layers.1.residual_connections.1         | ResidualConnection      | 2     \n","37  | model.encoder.layers.1.residual_connections.1.dropout | Dropout                 | 0     \n","38  | model.encoder.layers.1.residual_connections.1.norm    | LayerNormalization      | 2     \n","39  | model.encoder.layers.2                                | EncoderBlock            | 3.1 M \n","40  | model.encoder.layers.2.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n","41  | model.encoder.layers.2.self_attention_block.w_q       | Linear                  | 262 K \n","42  | model.encoder.layers.2.self_attention_block.w_k       | Linear                  | 262 K \n","43  | model.encoder.layers.2.self_attention_block.w_v       | Linear                  | 262 K \n","44  | model.encoder.layers.2.self_attention_block.w_o       | Linear                  | 262 K \n","45  | model.encoder.layers.2.self_attention_block.dropout   | Dropout                 | 0     \n","46  | model.encoder.layers.2.feed_forward_block             | FeedForwardBlock        | 2.1 M \n","47  | model.encoder.layers.2.feed_forward_block.linear_1    | Linear                  | 1.1 M \n","48  | model.encoder.layers.2.feed_forward_block.dropout     | Dropout                 | 0     \n","49  | model.encoder.layers.2.feed_forward_block.linear_2    | Linear                  | 1.0 M \n","50  | model.encoder.layers.2.residual_connections           | ModuleList              | 4     \n","51  | model.encoder.layers.2.residual_connections.0         | ResidualConnection      | 2     \n","52  | model.encoder.layers.2.residual_connections.0.dropout | Dropout                 | 0     \n","53  | model.encoder.layers.2.residual_connections.0.norm    | LayerNormalization      | 2     \n","54  | model.encoder.layers.2.residual_connections.1         | ResidualConnection      | 2     \n","55  | model.encoder.layers.2.residual_connections.1.dropout | Dropout                 | 0     \n","56  | model.encoder.layers.2.residual_connections.1.norm    | LayerNormalization      | 2     \n","57  | model.encoder.layers.3                                | EncoderBlock            | 3.1 M \n","58  | model.encoder.layers.3.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n","59  | model.encoder.layers.3.self_attention_block.w_q       | Linear                  | 262 K \n","60  | model.encoder.layers.3.self_attention_block.w_k       | Linear                  | 262 K \n","61  | model.encoder.layers.3.self_attention_block.w_v       | Linear                  | 262 K \n","62  | model.encoder.layers.3.self_attention_block.w_o       | Linear                  | 262 K \n","63  | model.encoder.layers.3.self_attention_block.dropout   | Dropout                 | 0     \n","64  | model.encoder.layers.3.feed_forward_block             | FeedForwardBlock        | 2.1 M \n","65  | model.encoder.layers.3.feed_forward_block.linear_1    | Linear                  | 1.1 M \n","66  | model.encoder.layers.3.feed_forward_block.dropout     | Dropout                 | 0     \n","67  | model.encoder.layers.3.feed_forward_block.linear_2    | Linear                  | 1.0 M \n","68  | model.encoder.layers.3.residual_connections           | ModuleList              | 4     \n","69  | model.encoder.layers.3.residual_connections.0         | ResidualConnection      | 2     \n","70  | model.encoder.layers.3.residual_connections.0.dropout | Dropout                 | 0     \n","71  | model.encoder.layers.3.residual_connections.0.norm    | LayerNormalization      | 2     \n","72  | model.encoder.layers.3.residual_connections.1         | ResidualConnection      | 2     \n","73  | model.encoder.layers.3.residual_connections.1.dropout | Dropout                 | 0     \n","74  | model.encoder.layers.3.residual_connections.1.norm    | LayerNormalization      | 2     \n","75  | model.encoder.layers.4                                | EncoderBlock            | 3.1 M \n","76  | model.encoder.layers.4.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n","77  | model.encoder.layers.4.self_attention_block.w_q       | Linear                  | 262 K \n","78  | model.encoder.layers.4.self_attention_block.w_k       | Linear                  | 262 K \n","79  | model.encoder.layers.4.self_attention_block.w_v       | Linear                  | 262 K \n","80  | model.encoder.layers.4.self_attention_block.w_o       | Linear                  | 262 K \n","81  | model.encoder.layers.4.self_attention_block.dropout   | Dropout                 | 0     \n","82  | model.encoder.layers.4.feed_forward_block             | FeedForwardBlock        | 2.1 M \n","83  | model.encoder.layers.4.feed_forward_block.linear_1    | Linear                  | 1.1 M \n","84  | model.encoder.layers.4.feed_forward_block.dropout     | Dropout                 | 0     \n","85  | model.encoder.layers.4.feed_forward_block.linear_2    | Linear                  | 1.0 M \n","86  | model.encoder.layers.4.residual_connections           | ModuleList              | 4     \n","87  | model.encoder.layers.4.residual_connections.0         | ResidualConnection      | 2     \n","88  | model.encoder.layers.4.residual_connections.0.dropout | Dropout                 | 0     \n","89  | model.encoder.layers.4.residual_connections.0.norm    | LayerNormalization      | 2     \n","90  | model.encoder.layers.4.residual_connections.1         | ResidualConnection      | 2     \n","91  | model.encoder.layers.4.residual_connections.1.dropout | Dropout                 | 0     \n","92  | model.encoder.layers.4.residual_connections.1.norm    | LayerNormalization      | 2     \n","93  | model.encoder.layers.5                                | EncoderBlock            | 3.1 M \n","94  | model.encoder.layers.5.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n","95  | model.encoder.layers.5.self_attention_block.w_q       | Linear                  | 262 K \n","96  | model.encoder.layers.5.self_attention_block.w_k       | Linear                  | 262 K \n","97  | model.encoder.layers.5.self_attention_block.w_v       | Linear                  | 262 K \n","98  | model.encoder.layers.5.self_attention_block.w_o       | Linear                  | 262 K \n","99  | model.encoder.layers.5.self_attention_block.dropout   | Dropout                 | 0     \n","100 | model.encoder.layers.5.feed_forward_block             | FeedForwardBlock        | 2.1 M \n","101 | model.encoder.layers.5.feed_forward_block.linear_1    | Linear                  | 1.1 M \n","102 | model.encoder.layers.5.feed_forward_block.dropout     | Dropout                 | 0     \n","103 | model.encoder.layers.5.feed_forward_block.linear_2    | Linear                  | 1.0 M \n","104 | model.encoder.layers.5.residual_connections           | ModuleList              | 4     \n","105 | model.encoder.layers.5.residual_connections.0         | ResidualConnection      | 2     \n","106 | model.encoder.layers.5.residual_connections.0.dropout | Dropout                 | 0     \n","107 | model.encoder.layers.5.residual_connections.0.norm    | LayerNormalization      | 2     \n","108 | model.encoder.layers.5.residual_connections.1         | ResidualConnection      | 2     \n","109 | model.encoder.layers.5.residual_connections.1.dropout | Dropout                 | 0     \n","110 | model.encoder.layers.5.residual_connections.1.norm    | LayerNormalization      | 2     \n","111 | model.encoder.norm                                    | LayerNormalization      | 2     \n","112 | model.decoder                                         | Decoder                 | 25.2 M\n","113 | model.decoder.layers                                  | ModuleList              | 25.2 M\n","114 | model.decoder.layers.0                                | DecoderBlock            | 4.2 M \n","115 | model.decoder.layers.0.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n","116 | model.decoder.layers.0.self_attention_block.w_q       | Linear                  | 262 K \n","117 | model.decoder.layers.0.self_attention_block.w_k       | Linear                  | 262 K \n","118 | model.decoder.layers.0.self_attention_block.w_v       | Linear                  | 262 K \n","119 | model.decoder.layers.0.self_attention_block.w_o       | Linear                  | 262 K \n","120 | model.decoder.layers.0.self_attention_block.dropout   | Dropout                 | 0     \n","121 | model.decoder.layers.0.cross_attention_block          | MultiHeadAttentionBlock | 1.0 M \n","122 | model.decoder.layers.0.cross_attention_block.w_q      | Linear                  | 262 K \n","123 | model.decoder.layers.0.cross_attention_block.w_k      | Linear                  | 262 K \n","124 | model.decoder.layers.0.cross_attention_block.w_v      | Linear                  | 262 K \n","125 | model.decoder.layers.0.cross_attention_block.w_o      | Linear                  | 262 K \n","126 | model.decoder.layers.0.cross_attention_block.dropout  | Dropout                 | 0     \n","127 | model.decoder.layers.0.feed_forward_block             | FeedForwardBlock        | 2.1 M \n","128 | model.decoder.layers.0.feed_forward_block.linear_1    | Linear                  | 1.1 M \n","129 | model.decoder.layers.0.feed_forward_block.dropout     | Dropout                 | 0     \n","130 | model.decoder.layers.0.feed_forward_block.linear_2    | Linear                  | 1.0 M \n","131 | model.decoder.layers.0.residual_connections           | ModuleList              | 6     \n","132 | model.decoder.layers.0.residual_connections.0         | ResidualConnection      | 2     \n","133 | model.decoder.layers.0.residual_connections.0.dropout | Dropout                 | 0     \n","134 | model.decoder.layers.0.residual_connections.0.norm    | LayerNormalization      | 2     \n","135 | model.decoder.layers.0.residual_connections.1         | ResidualConnection      | 2     \n","136 | model.decoder.layers.0.residual_connections.1.dropout | Dropout                 | 0     \n","137 | model.decoder.layers.0.residual_connections.1.norm    | LayerNormalization      | 2     \n","138 | model.decoder.layers.0.residual_connections.2         | ResidualConnection      | 2     \n","139 | model.decoder.layers.0.residual_connections.2.dropout | Dropout                 | 0     \n","140 | model.decoder.layers.0.residual_connections.2.norm    | LayerNormalization      | 2     \n","141 | model.decoder.layers.1                                | DecoderBlock            | 4.2 M \n","142 | model.decoder.layers.1.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n","143 | model.decoder.layers.1.self_attention_block.w_q       | Linear                  | 262 K \n","144 | model.decoder.layers.1.self_attention_block.w_k       | Linear                  | 262 K \n","145 | model.decoder.layers.1.self_attention_block.w_v       | Linear                  | 262 K \n","146 | model.decoder.layers.1.self_attention_block.w_o       | Linear                  | 262 K \n","147 | model.decoder.layers.1.self_attention_block.dropout   | Dropout                 | 0     \n","148 | model.decoder.layers.1.cross_attention_block          | MultiHeadAttentionBlock | 1.0 M \n","149 | model.decoder.layers.1.cross_attention_block.w_q      | Linear                  | 262 K \n","150 | model.decoder.layers.1.cross_attention_block.w_k      | Linear                  | 262 K \n","151 | model.decoder.layers.1.cross_attention_block.w_v      | Linear                  | 262 K \n","152 | model.decoder.layers.1.cross_attention_block.w_o      | Linear                  | 262 K \n","153 | model.decoder.layers.1.cross_attention_block.dropout  | Dropout                 | 0     \n","154 | model.decoder.layers.1.feed_forward_block             | FeedForwardBlock        | 2.1 M \n","155 | model.decoder.layers.1.feed_forward_block.linear_1    | Linear                  | 1.1 M \n","156 | model.decoder.layers.1.feed_forward_block.dropout     | Dropout                 | 0     \n","157 | model.decoder.layers.1.feed_forward_block.linear_2    | Linear                  | 1.0 M \n","158 | model.decoder.layers.1.residual_connections           | ModuleList              | 6     \n","159 | model.decoder.layers.1.residual_connections.0         | ResidualConnection      | 2     \n","160 | model.decoder.layers.1.residual_connections.0.dropout | Dropout                 | 0     \n","161 | model.decoder.layers.1.residual_connections.0.norm    | LayerNormalization      | 2     \n","162 | model.decoder.layers.1.residual_connections.1         | ResidualConnection      | 2     \n","163 | model.decoder.layers.1.residual_connections.1.dropout | Dropout                 | 0     \n","164 | model.decoder.layers.1.residual_connections.1.norm    | LayerNormalization      | 2     \n","165 | model.decoder.layers.1.residual_connections.2         | ResidualConnection      | 2     \n","166 | model.decoder.layers.1.residual_connections.2.dropout | Dropout                 | 0     \n","167 | model.decoder.layers.1.residual_connections.2.norm    | LayerNormalization      | 2     \n","168 | model.decoder.layers.2                                | DecoderBlock            | 4.2 M \n","169 | model.decoder.layers.2.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n","170 | model.decoder.layers.2.self_attention_block.w_q       | Linear                  | 262 K \n","171 | model.decoder.layers.2.self_attention_block.w_k       | Linear                  | 262 K \n","172 | model.decoder.layers.2.self_attention_block.w_v       | Linear                  | 262 K \n","173 | model.decoder.layers.2.self_attention_block.w_o       | Linear                  | 262 K \n","174 | model.decoder.layers.2.self_attention_block.dropout   | Dropout                 | 0     \n","175 | model.decoder.layers.2.cross_attention_block          | MultiHeadAttentionBlock | 1.0 M \n","176 | model.decoder.layers.2.cross_attention_block.w_q      | Linear                  | 262 K \n","177 | model.decoder.layers.2.cross_attention_block.w_k      | Linear                  | 262 K \n","178 | model.decoder.layers.2.cross_attention_block.w_v      | Linear                  | 262 K \n","179 | model.decoder.layers.2.cross_attention_block.w_o      | Linear                  | 262 K \n","180 | model.decoder.layers.2.cross_attention_block.dropout  | Dropout                 | 0     \n","181 | model.decoder.layers.2.feed_forward_block             | FeedForwardBlock        | 2.1 M \n","182 | model.decoder.layers.2.feed_forward_block.linear_1    | Linear                  | 1.1 M \n","183 | model.decoder.layers.2.feed_forward_block.dropout     | Dropout                 | 0     \n","184 | model.decoder.layers.2.feed_forward_block.linear_2    | Linear                  | 1.0 M \n","185 | model.decoder.layers.2.residual_connections           | ModuleList              | 6     \n","186 | model.decoder.layers.2.residual_connections.0         | ResidualConnection      | 2     \n","187 | model.decoder.layers.2.residual_connections.0.dropout | Dropout                 | 0     \n","188 | model.decoder.layers.2.residual_connections.0.norm    | LayerNormalization      | 2     \n","189 | model.decoder.layers.2.residual_connections.1         | ResidualConnection      | 2     \n","190 | model.decoder.layers.2.residual_connections.1.dropout | Dropout                 | 0     \n","191 | model.decoder.layers.2.residual_connections.1.norm    | LayerNormalization      | 2     \n","192 | model.decoder.layers.2.residual_connections.2         | ResidualConnection      | 2     \n","193 | model.decoder.layers.2.residual_connections.2.dropout | Dropout                 | 0     \n","194 | model.decoder.layers.2.residual_connections.2.norm    | LayerNormalization      | 2     \n","195 | model.decoder.layers.3                                | DecoderBlock            | 4.2 M \n","196 | model.decoder.layers.3.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n","197 | model.decoder.layers.3.self_attention_block.w_q       | Linear                  | 262 K \n","198 | model.decoder.layers.3.self_attention_block.w_k       | Linear                  | 262 K \n","199 | model.decoder.layers.3.self_attention_block.w_v       | Linear                  | 262 K \n","200 | model.decoder.layers.3.self_attention_block.w_o       | Linear                  | 262 K \n","201 | model.decoder.layers.3.self_attention_block.dropout   | Dropout                 | 0     \n","202 | model.decoder.layers.3.cross_attention_block          | MultiHeadAttentionBlock | 1.0 M \n","203 | model.decoder.layers.3.cross_attention_block.w_q      | Linear                  | 262 K \n","204 | model.decoder.layers.3.cross_attention_block.w_k      | Linear                  | 262 K \n","205 | model.decoder.layers.3.cross_attention_block.w_v      | Linear                  | 262 K \n","206 | model.decoder.layers.3.cross_attention_block.w_o      | Linear                  | 262 K \n","207 | model.decoder.layers.3.cross_attention_block.dropout  | Dropout                 | 0     \n","208 | model.decoder.layers.3.feed_forward_block             | FeedForwardBlock        | 2.1 M \n","209 | model.decoder.layers.3.feed_forward_block.linear_1    | Linear                  | 1.1 M \n","210 | model.decoder.layers.3.feed_forward_block.dropout     | Dropout                 | 0     \n","211 | model.decoder.layers.3.feed_forward_block.linear_2    | Linear                  | 1.0 M \n","212 | model.decoder.layers.3.residual_connections           | ModuleList              | 6     \n","213 | model.decoder.layers.3.residual_connections.0         | ResidualConnection      | 2     \n","214 | model.decoder.layers.3.residual_connections.0.dropout | Dropout                 | 0     \n","215 | model.decoder.layers.3.residual_connections.0.norm    | LayerNormalization      | 2     \n","216 | model.decoder.layers.3.residual_connections.1         | ResidualConnection      | 2     \n","217 | model.decoder.layers.3.residual_connections.1.dropout | Dropout                 | 0     \n","218 | model.decoder.layers.3.residual_connections.1.norm    | LayerNormalization      | 2     \n","219 | model.decoder.layers.3.residual_connections.2         | ResidualConnection      | 2     \n","220 | model.decoder.layers.3.residual_connections.2.dropout | Dropout                 | 0     \n","221 | model.decoder.layers.3.residual_connections.2.norm    | LayerNormalization      | 2     \n","222 | model.decoder.layers.4                                | DecoderBlock            | 4.2 M \n","223 | model.decoder.layers.4.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n","224 | model.decoder.layers.4.self_attention_block.w_q       | Linear                  | 262 K \n","225 | model.decoder.layers.4.self_attention_block.w_k       | Linear                  | 262 K \n","226 | model.decoder.layers.4.self_attention_block.w_v       | Linear                  | 262 K \n","227 | model.decoder.layers.4.self_attention_block.w_o       | Linear                  | 262 K \n","228 | model.decoder.layers.4.self_attention_block.dropout   | Dropout                 | 0     \n","229 | model.decoder.layers.4.cross_attention_block          | MultiHeadAttentionBlock | 1.0 M \n","230 | model.decoder.layers.4.cross_attention_block.w_q      | Linear                  | 262 K \n","231 | model.decoder.layers.4.cross_attention_block.w_k      | Linear                  | 262 K \n","232 | model.decoder.layers.4.cross_attention_block.w_v      | Linear                  | 262 K \n","233 | model.decoder.layers.4.cross_attention_block.w_o      | Linear                  | 262 K \n","234 | model.decoder.layers.4.cross_attention_block.dropout  | Dropout                 | 0     \n","235 | model.decoder.layers.4.feed_forward_block             | FeedForwardBlock        | 2.1 M \n","236 | model.decoder.layers.4.feed_forward_block.linear_1    | Linear                  | 1.1 M \n","237 | model.decoder.layers.4.feed_forward_block.dropout     | Dropout                 | 0     \n","238 | model.decoder.layers.4.feed_forward_block.linear_2    | Linear                  | 1.0 M \n","239 | model.decoder.layers.4.residual_connections           | ModuleList              | 6     \n","240 | model.decoder.layers.4.residual_connections.0         | ResidualConnection      | 2     \n","241 | model.decoder.layers.4.residual_connections.0.dropout | Dropout                 | 0     \n","242 | model.decoder.layers.4.residual_connections.0.norm    | LayerNormalization      | 2     \n","243 | model.decoder.layers.4.residual_connections.1         | ResidualConnection      | 2     \n","244 | model.decoder.layers.4.residual_connections.1.dropout | Dropout                 | 0     \n","245 | model.decoder.layers.4.residual_connections.1.norm    | LayerNormalization      | 2     \n","246 | model.decoder.layers.4.residual_connections.2         | ResidualConnection      | 2     \n","247 | model.decoder.layers.4.residual_connections.2.dropout | Dropout                 | 0     \n","248 | model.decoder.layers.4.residual_connections.2.norm    | LayerNormalization      | 2     \n","249 | model.decoder.layers.5                                | DecoderBlock            | 4.2 M \n","250 | model.decoder.layers.5.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n","251 | model.decoder.layers.5.self_attention_block.w_q       | Linear                  | 262 K \n","252 | model.decoder.layers.5.self_attention_block.w_k       | Linear                  | 262 K \n","253 | model.decoder.layers.5.self_attention_block.w_v       | Linear                  | 262 K \n","254 | model.decoder.layers.5.self_attention_block.w_o       | Linear                  | 262 K \n","255 | model.decoder.layers.5.self_attention_block.dropout   | Dropout                 | 0     \n","256 | model.decoder.layers.5.cross_attention_block          | MultiHeadAttentionBlock | 1.0 M \n","257 | model.decoder.layers.5.cross_attention_block.w_q      | Linear                  | 262 K \n","258 | model.decoder.layers.5.cross_attention_block.w_k      | Linear                  | 262 K \n","259 | model.decoder.layers.5.cross_attention_block.w_v      | Linear                  | 262 K \n","260 | model.decoder.layers.5.cross_attention_block.w_o      | Linear                  | 262 K \n","261 | model.decoder.layers.5.cross_attention_block.dropout  | Dropout                 | 0     \n","262 | model.decoder.layers.5.feed_forward_block             | FeedForwardBlock        | 2.1 M \n","263 | model.decoder.layers.5.feed_forward_block.linear_1    | Linear                  | 1.1 M \n","264 | model.decoder.layers.5.feed_forward_block.dropout     | Dropout                 | 0     \n","265 | model.decoder.layers.5.feed_forward_block.linear_2    | Linear                  | 1.0 M \n","266 | model.decoder.layers.5.residual_connections           | ModuleList              | 6     \n","267 | model.decoder.layers.5.residual_connections.0         | ResidualConnection      | 2     \n","268 | model.decoder.layers.5.residual_connections.0.dropout | Dropout                 | 0     \n","269 | model.decoder.layers.5.residual_connections.0.norm    | LayerNormalization      | 2     \n","270 | model.decoder.layers.5.residual_connections.1         | ResidualConnection      | 2     \n","271 | model.decoder.layers.5.residual_connections.1.dropout | Dropout                 | 0     \n","272 | model.decoder.layers.5.residual_connections.1.norm    | LayerNormalization      | 2     \n","273 | model.decoder.layers.5.residual_connections.2         | ResidualConnection      | 2     \n","274 | model.decoder.layers.5.residual_connections.2.dropout | Dropout                 | 0     \n","275 | model.decoder.layers.5.residual_connections.2.norm    | LayerNormalization      | 2     \n","276 | model.decoder.norm                                    | LayerNormalization      | 2     \n","277 | model.src_embed                                       | InputEmbeddings         | 8.0 M \n","278 | model.src_embed.embedding                             | Embedding               | 8.0 M \n","279 | model.tgt_embed                                       | InputEmbeddings         | 11.5 M\n","280 | model.tgt_embed.embedding                             | Embedding               | 11.5 M\n","281 | model.src_pos                                         | PositionalEncoding      | 0     \n","282 | model.src_pos.dropout                                 | Dropout                 | 0     \n","283 | model.tgt_pos                                         | PositionalEncoding      | 0     \n","284 | model.tgt_pos.dropout                                 | Dropout                 | 0     \n","285 | model.projection_layer                                | ProjectionLayer         | 11.5 M\n","286 | model.projection_layer.proj                           | Linear                  | 11.5 M\n","----------------------------------------------------------------------------------------------------\n","75.1 M    Trainable params\n","0         Non-trainable params\n","75.1 M    Total params\n","300.532   Total estimated model params size (MB)\n"]},{"output_type":"display_data","data":{"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d3a2dad379c43239a9690e544781ea9"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:Transformer:    SOURCE: Take her to bed.\"\n","INFO:Transformer:    TARGET: Andate a metterla a letto.\n","INFO:Transformer: PREDICTED: vitalità Sanina Sanina Sanina Sanina Sanina Sanina Sanina pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino intimai intimai intimai intimai intimai maggiormente pocolino intimai maggiormente pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino Volle consideriate consideriate pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino intimai Volle intimai Volle Saranno Saranno pocolino pocolino pocolino pocolino pocolino pocolino pocolino consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate pocolino Volle consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate avvocati Volle avvocati Volle pocolino pocolino pocolino pocolino pocolino avvocati consideriate consideriate consideriate avvocati avvocati Volle consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino avvocati Volle avvocati Volle pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino Volle Saranno Saranno Saranno Saranno Saranno pocolino pocolino pocolino Saranno Saranno Saranno Volle Volle Volle Volle Volle Volle Volle Volle Volle Volle Volle Volle Volle Volle Volle Volle Volle Volle Volle Volle Volle Volle consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate consideriate pocolino pocolino pocolino pocolino pocolino Volle consideriate consideriate Volle Volle Volle Volle Saranno pocolino pocolino pocolino pocolino pocolino Volle Volle Volle pocolino pocolino Volle Volle Volle rovinando Volle pocolino pocolino Volle Saranno Saranno Saranno Saranno Saranno pocolino pocolino Volle Volle Volle pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino pocolino Saranno Volle Volle Volle Volle Saranno Saranno Saranno Volle Volle Volle Volle Volle Volle Saranno Saranno Saranno Saranno Saranno Saranno Saranno Saranno Saranno\n","INFO:Transformer:--------------------\n","INFO:Transformer:  \n"]},{"output_type":"display_data","data":{"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39c7514e76114f778d5d23c4112e4d61"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loss Mean - 6.537288665771484\n","Loss Mean - 5.650068283081055\n"]}],"source":["# Monitor Learning rate while training to verify correct implementation of OCP\n","lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n","\n","# Save the last best model based on validation loss\n","checkpoint_callback = ModelCheckpoint(\n","    save_top_k=1,\n","    verbose=True,\n","    monitor='train_loss',\n","    mode='min',\n",")\n","\n","# Define trainer for model training\n","trainer = pl.Trainer(\n","    callbacks=[ModelSummary(max_depth=-1), lr_monitor, checkpoint_callback],\n","    max_epochs = 18,\n","    limit_val_batches=1,\n","    check_val_every_n_epoch=5,\n","    num_sanity_val_steps=1,\n","    precision=\"16-mixed\",\n","    accelerator=\"gpu\"\n",")\n","\n","# Train the Model\n","trainer.fit(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NA1MpMqmXcLT"},"outputs":[],"source":["# start tensorboard\n","%load_ext tensorboard\n","%tensorboard --logdir lightning_logs/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Alulvw6CRn9u"},"outputs":[],"source":["# Save the model\n","trainer.save_checkpoint(\"transformer.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vZ9Fy7_amRfw"},"outputs":[],"source":["trainer.validate()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d0c593a377984bc0ac68bb3d8fa0225b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a14dcc7202cf4a49a5950f65f2cacd8a","IPY_MODEL_a29fb6ca694a4af3b0d451accb12cd9b","IPY_MODEL_1c921d1bbe2a4ef1871f619751e67c29"],"layout":"IPY_MODEL_ec26cc15d19d4380a66f1348c0b3f6ae"}},"a14dcc7202cf4a49a5950f65f2cacd8a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1db5a93b4a1a4408aa4b77f1958bfb23","placeholder":"​","style":"IPY_MODEL_8a604d4faa044327bf9a87d7a050623e","value":"Downloading readme: 100%"}},"a29fb6ca694a4af3b0d451accb12cd9b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_129b1c06fc1d4e3fbd07352ae558ea58","max":28064,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0d6c22dcc6114cc585579399f8adbcce","value":28064}},"1c921d1bbe2a4ef1871f619751e67c29":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_076fcb0dbe3c46c8a5e62969a07266c1","placeholder":"​","style":"IPY_MODEL_7cd4ba7696d7454cbaaa7672edc08faf","value":" 28.1k/28.1k [00:00&lt;00:00, 2.14MB/s]"}},"ec26cc15d19d4380a66f1348c0b3f6ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1db5a93b4a1a4408aa4b77f1958bfb23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a604d4faa044327bf9a87d7a050623e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"129b1c06fc1d4e3fbd07352ae558ea58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d6c22dcc6114cc585579399f8adbcce":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"076fcb0dbe3c46c8a5e62969a07266c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cd4ba7696d7454cbaaa7672edc08faf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7839011b87f47b8b1a969fc2c29c959":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_defe3e10fae3405c8432e4a75c8adf36","IPY_MODEL_57410fc14d0d4e7eb8216b9a4853070c","IPY_MODEL_d09e87d610b249ff98b23b2b8a6ca527"],"layout":"IPY_MODEL_75ef97bef3b2439392a58c2613b5b3e7"}},"defe3e10fae3405c8432e4a75c8adf36":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d0d9d55cb584d648dee1735970370f8","placeholder":"​","style":"IPY_MODEL_d6febfc9cfa747bbacfb7c46cb2148bf","value":"Downloading data: 100%"}},"57410fc14d0d4e7eb8216b9a4853070c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_23107d08b809488cb886fdfb746446b2","max":5726189,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5446b01d32a24c96bb1ae3254442f645","value":5726189}},"d09e87d610b249ff98b23b2b8a6ca527":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6fd9b85203a41178ee625f241893c84","placeholder":"​","style":"IPY_MODEL_af54adbdcfaa4aecaa918cc3675a4c7e","value":" 5.73M/5.73M [00:00&lt;00:00, 13.9MB/s]"}},"75ef97bef3b2439392a58c2613b5b3e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d0d9d55cb584d648dee1735970370f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6febfc9cfa747bbacfb7c46cb2148bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"23107d08b809488cb886fdfb746446b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5446b01d32a24c96bb1ae3254442f645":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f6fd9b85203a41178ee625f241893c84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af54adbdcfaa4aecaa918cc3675a4c7e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f007c7e054664ede8328f343346aa808":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_81281a22d5414dbcb1985acd5d934da5","IPY_MODEL_838a1cbdd0db454eb991934039a70be9","IPY_MODEL_bc0b8b0f34b64402bfe0afb5eb41c202"],"layout":"IPY_MODEL_fbc847307db246beb77f71b1292e9010"}},"81281a22d5414dbcb1985acd5d934da5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e076052d62914452bae270677e31cd59","placeholder":"​","style":"IPY_MODEL_19a46868b2444eb4869b16cf346e4d3e","value":"Generating train split: 100%"}},"838a1cbdd0db454eb991934039a70be9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f78ecd1aa75144aa86e5330fb072dd46","max":32332,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6893b61500724731b8064b6f1441f651","value":32332}},"bc0b8b0f34b64402bfe0afb5eb41c202":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe411b0dfdf94fe7aba649c28148c928","placeholder":"​","style":"IPY_MODEL_684c5246fe6f4f8dac202c5b1514d5e6","value":" 32332/32332 [00:00&lt;00:00, 329486.14 examples/s]"}},"fbc847307db246beb77f71b1292e9010":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e076052d62914452bae270677e31cd59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19a46868b2444eb4869b16cf346e4d3e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f78ecd1aa75144aa86e5330fb072dd46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6893b61500724731b8064b6f1441f651":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fe411b0dfdf94fe7aba649c28148c928":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"684c5246fe6f4f8dac202c5b1514d5e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d3a2dad379c43239a9690e544781ea9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9e77971ee2e2438c9badb8da3e6bccea","IPY_MODEL_94a42c5391884245b356645f4cc1bcce","IPY_MODEL_7213a68c5d9448deb6f4f74e71a3d672"],"layout":"IPY_MODEL_099e58da9061422b9e61cc8013800173"}},"9e77971ee2e2438c9badb8da3e6bccea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0437f15650b345058b57c23047c79f36","placeholder":"​","style":"IPY_MODEL_db41403862d241d5aff26b42d7a2ea6d","value":"Sanity Checking DataLoader 0: 100%"}},"94a42c5391884245b356645f4cc1bcce":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f0ee045172f47bd8153f6634f611b78","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2c49c953ada04b0b9dbf9d2e5cf42db5","value":1}},"7213a68c5d9448deb6f4f74e71a3d672":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a7995d8bdd84781a553c31fade773f7","placeholder":"​","style":"IPY_MODEL_a7af00a888c34b95ad28fea34d3cf4d2","value":" 1/1 [00:05&lt;00:00,  0.19it/s]"}},"099e58da9061422b9e61cc8013800173":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"0437f15650b345058b57c23047c79f36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db41403862d241d5aff26b42d7a2ea6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f0ee045172f47bd8153f6634f611b78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c49c953ada04b0b9dbf9d2e5cf42db5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5a7995d8bdd84781a553c31fade773f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7af00a888c34b95ad28fea34d3cf4d2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"39c7514e76114f778d5d23c4112e4d61":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c96443ab9a7245e5ac17c4ff203cb979","IPY_MODEL_3cdb589de67b4c21a961a901cead2b35","IPY_MODEL_ad0db756198e4dd1a37ca9326add3796"],"layout":"IPY_MODEL_cfc7f9a927624aa7ab0a3c60b5bce3b1"}},"c96443ab9a7245e5ac17c4ff203cb979":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c68799d28b5c41eca40f1123ce310b90","placeholder":"​","style":"IPY_MODEL_c2f7df85960649dc827a451658e17379","value":"Epoch 1: 100%"}},"3cdb589de67b4c21a961a901cead2b35":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8a171c747c94287a8278af90f416d2c","max":4850,"min":0,"orientation":"horizontal","style":"IPY_MODEL_71fa4dbb11464937a2198862745b3e7b","value":4850}},"ad0db756198e4dd1a37ca9326add3796":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9997997b56341f882bc9c9d14cf776d","placeholder":"​","style":"IPY_MODEL_eb54799ccfc14e3da35dca0539de3eef","value":" 4850/4850 [08:46&lt;00:00,  9.22it/s, v_num=3, train_loss=5.650]"}},"cfc7f9a927624aa7ab0a3c60b5bce3b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"c68799d28b5c41eca40f1123ce310b90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2f7df85960649dc827a451658e17379":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c8a171c747c94287a8278af90f416d2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71fa4dbb11464937a2198862745b3e7b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e9997997b56341f882bc9c9d14cf776d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb54799ccfc14e3da35dca0539de3eef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}